#!/bin/bash

# Script para integrar modelos de Ollama en Cursor

echo "ðŸ”— Integrando modelos de Ollama en Cursor..."

# Colores
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m'

print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

# 1. Crear configuraciÃ³n avanzada de Cursor con modelos locales
print_status "Configurando modelos locales en Cursor..."

cat > ~/.cursor/settings.json << 'EOF'
{
  "ai.autoDetectProvider": true,
  "ai.fallbackProvider": "ollama",
  "ai.onlineProvider": "openai",
  "ai.offlineProvider": "ollama",
  "ai.ollamaEndpoint": "http://localhost:11434/api/generate",
  "ai.ollamaModels": [
    {
      "name": "dolphin-mixtral:latest",
      "displayName": "Dolphin Mixtral (26GB) - Mejor calidad",
      "description": "Modelo mÃ¡s potente para tareas complejas",
      "category": "general"
    },
    {
      "name": "codellama:13b",
      "displayName": "CodeLlama 13B (7.4GB) - CÃ³digo",
      "description": "Especializado en generaciÃ³n de cÃ³digo",
      "category": "coding"
    },
    {
      "name": "llama3:latest",
      "displayName": "Llama 3 (4.7GB) - General",
      "description": "Modelo equilibrado para tareas generales",
      "category": "general"
    },
    {
      "name": "zephyr:latest",
      "displayName": "Zephyr (4.1GB) - RÃ¡pido",
      "description": "Respuestas rÃ¡pidas para tareas simples",
      "category": "fast"
    },
    {
      "name": "deepseek-coder:6.7b-instruct",
      "displayName": "DeepSeek Coder (3.8GB) - CÃ³digo",
      "description": "Especializado en tareas de programaciÃ³n",
      "category": "coding"
    }
  ],
  "ai.defaultOfflineModel": "dolphin-mixtral:latest",
  "ai.modelSelection": {
    "auto": true,
    "coding": "deepseek-coder:6.7b-instruct",
    "general": "dolphin-mixtral:latest",
    "fast": "zephyr:latest"
  },
  "ai.showLocalModels": true,
  "ai.localModelsFirst": true,
  "ai.connectionTimeout": 3000,
  "ai.offlineDetection": {
    "enabled": true,
    "timeout": 3000,
    "fallback": "ollama"
  }
}
EOF

# 2. Crear script que sincronice modelos con Cursor
print_status "Creando sincronizador de modelos..."

cat > ~/.cursor/sync-models.sh << 'EOF'
#!/bin/bash

# Script para sincronizar modelos de Ollama con Cursor

echo "ðŸ”„ Sincronizando modelos de Ollama..."

# Obtener modelos disponibles
MODELS=$(curl -s http://localhost:11434/api/tags 2>/dev/null | jq -r '.models[].name' 2>/dev/null)

if [ -z "$MODELS" ]; then
    echo "âŒ No se pueden obtener modelos de Ollama"
    exit 1
fi

echo "ðŸ“‹ Modelos disponibles en Ollama:"
echo "$MODELS"

# Crear archivo de configuraciÃ³n dinÃ¡mica
cat > ~/.cursor/ollama-models.json << 'MODELCONFIG'
{
  "models": [
MODELCONFIG

# Agregar cada modelo
FIRST=true
echo "$MODELS" | while read -r model; do
    if [ "$FIRST" = true ]; then
        FIRST=false
    else
        echo "    ," >> ~/.cursor/ollama-models.json
    fi
    
    # Determinar categorÃ­a basada en el nombre
    CATEGORY="general"
    if [[ "$model" == *"code"* ]] || [[ "$model" == *"coder"* ]]; then
        CATEGORY="coding"
    elif [[ "$model" == *"zephyr"* ]]; then
        CATEGORY="fast"
    fi
    
    # Determinar tamaÃ±o aproximado
    SIZE="Unknown"
    if [[ "$model" == *"dolphin-mixtral"* ]]; then
        SIZE="26GB"
    elif [[ "$model" == *"codellama:13b"* ]]; then
        SIZE="7.4GB"
    elif [[ "$model" == *"llama3"* ]]; then
        SIZE="4.7GB"
    elif [[ "$model" == *"zephyr"* ]]; then
        SIZE="4.1GB"
    elif [[ "$model" == *"deepseek-coder"* ]]; then
        SIZE="3.8GB"
    fi
    
    cat >> ~/.cursor/ollama-models.json << EOF
    {
      "name": "$model",
      "displayName": "$model ($SIZE)",
      "description": "Modelo local de Ollama",
      "category": "$CATEGORY",
      "size": "$SIZE",
      "local": true
    }
EOF
done

echo "  ]" >> ~/.cursor/ollama-models.json
echo "}" >> ~/.cursor/ollama-models.json

echo "âœ… Modelos sincronizados con Cursor"
echo "ðŸ“ ConfiguraciÃ³n guardada en: ~/.cursor/ollama-models.json"
EOF

chmod +x ~/.cursor/sync-models.sh

# 3. Crear script de inicio mejorado
print_status "Mejorando script de inicio..."

cat > ~/.cursor/start-cursor-enhanced.sh << 'EOF'
#!/bin/bash

# Script mejorado para iniciar Cursor con modelos locales

echo "ðŸ” Detectando conexiÃ³n y modelos..."

# Verificar conexiÃ³n
if curl -s --connect-timeout 3 https://www.google.com > /dev/null 2>&1; then
    echo "âœ… Conectado a internet - usando modelos en la nube"
    # No configurar Ollama - usar modelos por defecto
    unset OLLAMA_HOST
    unset CURSOR_AI_PROVIDER
    unset CURSOR_AI_ENDPOINT
else
    echo "ðŸŒ Sin conexiÃ³n - configurando Ollama local..."
    
    # Iniciar Ollama si no estÃ¡ corriendo
    if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "ðŸš€ Iniciando Ollama..."
        ollama serve > /dev/null 2>&1 &
        sleep 5
    fi
    
    # Sincronizar modelos
    if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
        echo "ðŸ”„ Sincronizando modelos locales..."
        ~/.cursor/sync-models.sh
    fi
    
    # Configurar para usar Ollama
    export OLLAMA_HOST=http://localhost:11434
    export CURSOR_AI_PROVIDER=ollama
    export CURSOR_AI_ENDPOINT=http://localhost:11434/api/generate
    
    echo "ðŸ¤– Ollama configurado con modelos locales"
fi

# Iniciar Cursor
echo "ðŸš€ Iniciando Cursor..."
open -a Cursor
EOF

chmod +x ~/.cursor/start-cursor-enhanced.sh

# 4. Actualizar alias
print_status "Actualizando alias..."

# Actualizar .zshrc
if grep -q "cursor=" ~/.zshrc; then
    # Reemplazar alias existente
    sed -i '' 's|alias cursor=.*|alias cursor="~/.cursor/start-cursor-enhanced.sh"|' ~/.zshrc
else
    # Agregar nuevo alias
    echo "" >> ~/.zshrc
    echo "# Cursor con modelos locales" >> ~/.zshrc
    echo 'alias cursor="~/.cursor/start-cursor-enhanced.sh"' >> ~/.zshrc
fi

# 5. Crear comando para mostrar modelos
print_status "Creando comando para listar modelos..."

cat > ~/.cursor/list-models.sh << 'EOF'
#!/bin/bash

# Script para listar modelos disponibles

echo "ðŸ” Modelos disponibles:"
echo ""

# Verificar conexiÃ³n
if curl -s --connect-timeout 3 https://www.google.com > /dev/null 2>&1; then
    echo "âœ… CON INTERNET:"
    echo "   - GPT-4 (mÃ¡s rÃ¡pido, requiere tokens)"
    echo "   - GPT-3.5-turbo (rÃ¡pido, requiere tokens)"
    echo ""
fi

# Verificar Ollama
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "ðŸ¤– MODELOS LOCALES (Ollama):"
    ollama list --format "table {{.Name}}\t{{.Size}}\t{{.ModifiedAt}}"
    echo ""
    echo "ðŸ’¡ Para usar modelos locales:"
    echo "   1. Desconecta internet"
    echo "   2. Ejecuta: cursor"
    echo "   3. Los modelos aparecerÃ¡n en la lista de Cursor"
else
    echo "âš ï¸  Ollama no estÃ¡ corriendo"
    echo "   Ejecuta: ollama serve"
fi
EOF

chmod +x ~/.cursor/list-models.sh

# 6. Probar configuraciÃ³n
print_status "Probando configuraciÃ³n..."

# Verificar Ollama
if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    print_success "Ollama estÃ¡ funcionando"
    ~/.cursor/sync-models.sh
else
    print_warning "Ollama no estÃ¡ corriendo - ejecuta: ollama serve"
fi

echo ""
echo "=========================================="
print_success "Â¡IntegraciÃ³n de modelos completada!"
echo "=========================================="
echo ""
echo "ðŸŽ¯ Ahora cuando estÃ©s offline:"
echo ""
echo "1. Ejecuta: cursor"
echo "2. En Cursor, al usar Cmd+Shift+L:"
echo "   - VerÃ¡s tus modelos locales en la lista"
echo "   - PodrÃ¡s elegir entre:"
echo "     â€¢ dolphin-mixtral:latest (26GB) - Mejor calidad"
echo "     â€¢ codellama:13b (7.4GB) - CÃ³digo"
echo "     â€¢ llama3:latest (4.7GB) - General"
echo "     â€¢ zephyr:latest (4.1GB) - RÃ¡pido"
echo "     â€¢ deepseek-coder:6.7b-instruct (3.8GB) - CÃ³digo"
echo ""
echo "ðŸš€ Comandos disponibles:"
echo "   cursor              - Inicia Cursor con detecciÃ³n automÃ¡tica"
echo "   ~/.cursor/list-models.sh - Muestra modelos disponibles"
echo "   ~/.cursor/sync-models.sh - Sincroniza modelos con Cursor"
echo ""
print_success "Â¡Tus modelos locales aparecerÃ¡n en Cursor! ðŸŽ‰"
