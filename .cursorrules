# Cursor Rules for Local Ollama Integration

## Local AI Configuration
This project is configured to use local Ollama models for AI assistance when offline or to avoid using tokens.

### Available Models
- **dolphin-mixtral:latest** (26 GB) - Best general purpose model
- **codellama:13b** (7.4 GB) - Excellent for coding tasks
- **llama3:latest** (4.7 GB) - Good general purpose model
- **zephyr:latest** (4.1 GB) - Fast and efficient
- **deepseek-coder:6.7b-instruct** (3.8 GB) - Specialized for coding

### How to Use Local AI in Cursor

1. **Start Ollama Server**:
   ```bash
   ollama serve
   ```

2. **Verify Models are Available**:
   ```bash
   ollama list
   ```

3. **Access Local AI Chat**:
   - Navigate to `/ollama` in your application
   - Or use the API endpoints directly

### API Endpoints

- `GET /api/ollama/status` - Check Ollama connection
- `GET /api/ollama/models` - List available models
- `GET /api/ollama/best-model` - Get best general model
- `GET /api/ollama/best-coding-model` - Get best coding model
- `POST /api/ollama/generate` - Generate response
- `POST /api/ollama/generate-stream` - Stream response
- `POST /api/ollama/pull` - Download new model
- `DELETE /api/ollama/delete` - Delete model

### Cursor Integration

To use Ollama with Cursor:

1. **Install Cursor Extension** (if available) for Ollama integration
2. **Configure Cursor** to use local Ollama endpoint: `http://localhost:11434`
3. **Set Environment Variables**:
   ```bash
   export OLLAMA_HOST=http://localhost:11434
   export CURSOR_AI_PROVIDER=ollama
   ```

### Offline Usage

When working offline:
1. Ensure Ollama is running locally
2. Models are already downloaded
3. Use the local chat interface at `/ollama`
4. All AI interactions will be processed locally

### Model Recommendations

- **For General Development**: Use `dolphin-mixtral:latest` (best quality)
- **For Code Generation**: Use `deepseek-coder:6.7b-instruct` or `codellama:13b`
- **For Fast Responses**: Use `zephyr:latest`
- **For Balanced Performance**: Use `llama3:latest`

### Performance Tips

1. **Memory Management**: Close unused models to free memory
2. **Model Selection**: Choose smaller models for faster responses
3. **Batch Processing**: Group related queries together
4. **Streaming**: Use streaming for long responses to see progress

### Troubleshooting

- **Connection Issues**: Ensure Ollama is running on port 11434
- **Model Not Found**: Pull the model using `ollama pull <model-name>`
- **Slow Responses**: Try a smaller model or check system resources
- **Memory Issues**: Close other applications or use smaller models
